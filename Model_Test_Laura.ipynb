{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Model_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HelenNunez/BALG_Project3/blob/master/Model_Test_Laura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIs6qlVFFA0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0f9df8c9-cfdb-46d8-d68b-6d63a2001490"
      },
      "source": [
        "!pip install findspark\n",
        "# # Start a SparkSession\n",
        "\n",
        "# # Install Java, Spark, and Findspark\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.4.2)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fKT52otFYFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f28b697b-c2ce-450d-f792-9fc200071519"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXk8goKRFxy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0657abdc-e52c-4552-e230-5edcf3310fd0"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/BALG_GoogleDriveLN -r /content/ "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot open '/content/drive/My Drive/BALG_GoogleDriveLN/trump_sentiment_new.gsheet' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/My Drive/BALG_GoogleDriveLN/sentiment_sentiment_new.gsheet' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/My Drive/BALG_GoogleDriveLN/biden_sentiment_For_SR (1).csv/positive biden.gsheet' for reading: Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVwqspCCFA0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import glob\n",
        "import pandas as pd\n",
        "import findspark\n",
        "import json\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "\n",
        "findspark.init()\n",
        "import pyspark\n",
        "\n",
        "from pyspark import SparkContext, SparkFiles\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructField, StringType, StructType, ArrayType\n",
        "from pyspark.sql.functions import format_number as fmt\n",
        "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
        "from pyspark.sql.functions import udf, avg, col\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textblob import TextBlob"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uohZpcYKFA0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "907eec96-1e7b-42d3-aaa8-689e2fa6c7e8"
      },
      "source": [
        "# Read Trump and Biden Data File and store into Pandas Data Frames\n",
        "trump_data = pd.read_csv('./BALG_GoogleDriveLN/trump_reddit_pull_test.csv')\n",
        "biden_data = pd.read_csv('./BALG_GoogleDriveLN/biden_reddit_pull_test.csv')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tEPVmtYGaco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "ab0fb072-bc5d-4929-deff-5efa527100f2"
      },
      "source": [
        "trump_data.head()\n",
        "trump_data.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>associated_award</th>\n",
              "      <th>author</th>\n",
              "      <th>author_fullname</th>\n",
              "      <th>author_premium</th>\n",
              "      <th>awarders</th>\n",
              "      <th>body</th>\n",
              "      <th>collapsed_because_crowd_control</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>gildings</th>\n",
              "      <th>id</th>\n",
              "      <th>is_submitter</th>\n",
              "      <th>link_id</th>\n",
              "      <th>locked</th>\n",
              "      <th>no_follow</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>permalink</th>\n",
              "      <th>retrieved_on</th>\n",
              "      <th>score</th>\n",
              "      <th>send_replies</th>\n",
              "      <th>stickied</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>subreddit_id</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>created</th>\n",
              "      <th>edited</th>\n",
              "      <th>author_cakeday</th>\n",
              "      <th>distinguished</th>\n",
              "      <th>treatment_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>84735</th>\n",
              "      <td>84735</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AutoModerator</td>\n",
              "      <td>t2_6l4z3</td>\n",
              "      <td>True</td>\n",
              "      <td>[]</td>\n",
              "      <td>\\nThis subreddit is a pro-Trump subreddit for ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1598141044</td>\n",
              "      <td>{}</td>\n",
              "      <td>g2j85bk</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_ietdnt</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>t3_ietdnt</td>\n",
              "      <td>/r/trump/comments/ietdnt/snopes_once_again_dec...</td>\n",
              "      <td>1598145630</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>trump</td>\n",
              "      <td>t5_2r18q</td>\n",
              "      <td>0</td>\n",
              "      <td>1.598141e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>moderator</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84736</th>\n",
              "      <td>84736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ConsistentPayment</td>\n",
              "      <td>t2_28k5wtcf</td>\n",
              "      <td>False</td>\n",
              "      <td>[]</td>\n",
              "      <td>&amp;gt; If people think all politicians are corru...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1598141017</td>\n",
              "      <td>{}</td>\n",
              "      <td>g2j83pu</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_iervb6</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>t1_g2j3y5u</td>\n",
              "      <td>/r/preppers/comments/iervb6/winter_is_coming/g...</td>\n",
              "      <td>1598145608</td>\n",
              "      <td>11</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>preppers</td>\n",
              "      <td>t5_2riow</td>\n",
              "      <td>0</td>\n",
              "      <td>1.598141e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84737</th>\n",
              "      <td>84737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AutoModerator</td>\n",
              "      <td>t2_6l4z3</td>\n",
              "      <td>True</td>\n",
              "      <td>[]</td>\n",
              "      <td>\\nThis subreddit is a pro-Trump subreddit for ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1598140981</td>\n",
              "      <td>{}</td>\n",
              "      <td>g2j81ge</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_ietd20</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>t3_ietd20</td>\n",
              "      <td>/r/trump/comments/ietd20/and_the_left_calls_tr...</td>\n",
              "      <td>1598145575</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>trump</td>\n",
              "      <td>t5_2r18q</td>\n",
              "      <td>0</td>\n",
              "      <td>1.598141e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>moderator</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84738</th>\n",
              "      <td>84738</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AutoModerator</td>\n",
              "      <td>t2_6l4z3</td>\n",
              "      <td>True</td>\n",
              "      <td>[]</td>\n",
              "      <td>\\nThis subreddit is a pro-Trump subreddit for ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1598140942</td>\n",
              "      <td>{}</td>\n",
              "      <td>g2j7z4m</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_ietcqe</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>t3_ietcqe</td>\n",
              "      <td>/r/trump/comments/ietcqe/this_guy_gets_it/g2j7...</td>\n",
              "      <td>1598145544</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>trump</td>\n",
              "      <td>t5_2r18q</td>\n",
              "      <td>0</td>\n",
              "      <td>1.598141e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>moderator</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84739</th>\n",
              "      <td>84739</td>\n",
              "      <td>NaN</td>\n",
              "      <td>technicallycorrect2</td>\n",
              "      <td>t2_14lcb9</td>\n",
              "      <td>True</td>\n",
              "      <td>[]</td>\n",
              "      <td>I heard donald trump was going to rake the pin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1598140924</td>\n",
              "      <td>{}</td>\n",
              "      <td>g2j7y15</td>\n",
              "      <td>False</td>\n",
              "      <td>t3_ies3bd</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>t1_g2j7s96</td>\n",
              "      <td>/r/news/comments/ies3bd/california_wildfires_a...</td>\n",
              "      <td>1598145528</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>news</td>\n",
              "      <td>t5_2qh3l</td>\n",
              "      <td>0</td>\n",
              "      <td>1.598141e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  associated_award  ... distinguished treatment_tags\n",
              "84735       84735               NaN  ...     moderator             []\n",
              "84736       84736               NaN  ...           NaN             []\n",
              "84737       84737               NaN  ...     moderator             []\n",
              "84738       84738               NaN  ...     moderator             []\n",
              "84739       84739               NaN  ...           NaN             []\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j24JJw4MG8hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biden_data['date'] = biden_data['retrieved_on'].apply(lambda x:datetime.datetime.fromtimestamp(x))  \n",
        "#New_Data['Date']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwMITfaVG9z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biden_data['just_date'] = biden_data['date'].dt.date"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89lbtdoTHCsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combine the data into a single dataset\n",
        "biden_body_df = biden_data[['date', 'just_date','body']] "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEgk6EtBHKJp",
        "colab_type": "text"
      },
      "source": [
        "# Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH5DOOGuFA0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23d7e741-040a-4e21-90d5-4df48a27eea7"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_hKXeGLwJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1M47PL9FA0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext(\"local\", \"Model\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G90y16YJ029",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ba1facce-c07e-49f9-c69d-9a875de1a3ac"
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.master', 'local'),\n",
              " ('spark.app.name', 'Model'),\n",
              " ('spark.app.id', 'local-1598905348254'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.driver.port', '38561'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.host', 'b3b963fc9d83'),\n",
              " ('spark.ui.showConsoleProgress', 'true')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDPG-3slMnds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fa1d01d-4338-4fb3-ef44-f769099331e5"
      },
      "source": [
        "SparkContext.setSystemProperty('spark.executor.memory','70g')\n",
        "SparkContext.setSystemProperty('spark.driver.memory','70g')\n",
        "SparkContext.setSystemProperty('spark.driver.maxResultSize','0')\n",
        "sc._conf.getAll()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
            "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
            "py4j.protocol.Py4JNetworkError: Error while receiving\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:40443)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Py4JNetworkError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ef612e5a5858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'70g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.memory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'70g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.maxResultSize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36msetSystemProperty\u001b[0;34m(cls, key, value)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:40443)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oZ-Z8xXFA0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#appName = \"Model\"\n",
        "#master = \"local\"\n",
        "#spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9gR-lmPFA07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SparkContext.stop(sc)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLfQVQoSFA1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bdf403e-0818-4f14-f503-ea1a06b469c9"
      },
      "source": [
        "model = Word2VecModel.load('./BALG_GoogleDriveLN/Models/one_million_twoptone')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1442d5eeff53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./BALG_GoogleDriveLN/Models/one_million_twoptone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 1 times, most recent failure: Lost task 3.0 in stage 2.0 (TID 5, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:562)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:258)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n\tat org.apache.spark.ml.feature.Word2VecModel$Word2VecModelReader.load(Word2Vec.scala:406)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:562)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:258)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4ZYoojSFA03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diction = model.getVectors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcr-1VmSHHUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "fc800a8b-30ff-4f97-98af-625c3a6aac4d"
      },
      "source": [
        "biden_df = spark.createDataFrame(biden_body_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c8dd237f4c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbiden_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiden_body_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDCqO_q4FA1F",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c99bbd3-f913-4544-e845-e1bd31a94767"
      },
      "source": [
        "cosine_similarity([[1,0,-1]],[[-1, -1, 0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iON1id2BFA1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = \"goodness\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QVxRrUXFA1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = [\"trump is so great. I love him\", \"Joe Biden for the Win\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXvrcI7GFA1P",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3ebc83f-c90a-45d2-d38e-b413a0099c8f"
      },
      "source": [
        "diction.collect()[diction[\"word\"] == \"alligator\"][\"vector\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not Column",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b1fa2763bfa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"alligator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vector\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not Column"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk5bpmnxFA1T",
        "colab_type": "code",
        "colab": {},
        "outputId": "b886c659-3953-4b1e-d798-984bdd78123a"
      },
      "source": [
        "diction.first().word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cristino'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbSFW6YFFA1W",
        "colab_type": "code",
        "colab": {},
        "outputId": "fda872a5-47d5-4768-fd91-4a51597ff37a"
      },
      "source": [
        "diction.first().vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-0.1161, -0.1775, 0.1231, -0.0801, 0.0813, 0.1487, -0.0536, 0.2583, -0.0229, 0.0011, -0.2627, 0.0232, 0.0512, -0.0504, -0.033, -0.0342, 0.088, 0.095, -0.0076, 0.0302, 0.0347, -0.0656, 0.0552, 0.1039, -0.1076, 0.0647, -0.0177, -0.0108, 0.1604, 0.1316, -0.0866, -0.0141, 0.0339, -0.0128, 0.0561, -0.0103, -0.0601, -0.0728, 0.0823, -0.0641, 0.0555, -0.1415, -0.1054, -0.0805, 0.1119, -0.0278, -0.0534, -0.0268, 0.0846, 0.1385, -0.0235, -0.1821, -0.1792, -0.0029, 0.2352, 0.0786, -0.242, -0.0798, -0.1995, 0.0964, -0.074, -0.1611, -0.0533, 0.121, 0.1041, -0.0404, 0.157, 0.1125, 0.0538, 0.0526, -0.0637, -0.0131, 0.0377, 0.0803, 0.1078, 0.0374, -0.0204, -0.1541, 0.0511, -0.014, -0.1813, 0.0673, 0.1497, -0.0247, 0.1128, 0.0413, 0.0873, -0.0177, -0.0465, 0.0495, -0.0844, -0.037, -0.0349, 0.0561, 0.1187, -0.09, -0.0318, 0.1475, -0.0955, -0.08, -0.0762, 0.0489, 0.0807, 0.0814, 0.0452, 0.0237, 0.1183, -0.0671, -0.1631, -0.0766, -0.0991, 0.0876, 0.0043, 0.0337, 0.0365, -0.0859, 0.0742, 0.0371, 0.1133, -0.0688, -0.0073, -0.1678, 0.2072, 0.1719, 0.1887, 0.0486, 0.1065, 0.125, 0.0159, -0.0299, -0.0966, 0.0198, -0.0551, 0.0781, 0.1474, -0.033, -0.0497, 0.0525, 0.1782, -0.037, 0.0417, 0.0666, 0.0475, 0.1532, -0.0013, 0.0083, 0.0964, -0.0164, -0.0057, -0.0303, -0.0361, 0.0179, 0.2038, -0.02, 0.0498, -0.0981, 0.0275, 0.1532, -0.0523, -0.1275, -0.042, 0.0072, -0.1868, 0.1111, 0.1146, -0.1983, 0.1555, 0.0803, 0.0611, 0.0271, 0.0653, -0.1761, -0.0638, 0.0019, -0.1481, -0.0195, -0.202, -0.1273, -0.1629, 0.102, -0.0354, -0.0741, 0.0317, 0.0481, 0.1887, 0.0217, 0.0764, -0.0517, 0.0138, 0.198, 0.0826, -0.0876, 0.0564, 0.0299, 0.0419, -0.0538, 0.1658, 0.0341, 0.0987, 0.0808, 0.1717, -0.0918, -0.0363, 0.0243, -0.0075, 0.112, -0.0106, 0.0505, -0.0388, 0.0504, 0.1889, -0.1394, 0.0657, -0.0963, 0.1141, -0.1223, 0.0565, -0.1725, 0.118, -0.0263, 0.0118, 0.1636, 0.0295, 0.0099, 0.0757, -0.0419, -0.0919, -0.0431, -0.0433, 0.116, 0.0296, 0.1356, 0.1018, -0.1824, -0.0747, -0.0046, 0.0229, 0.0917, -0.1192, -0.0634, -0.0213, 0.0053, 0.1005, 0.1838, -0.0454, -0.027, -0.0674, -0.0035, -0.099, 0.0037, -0.0863, -0.0261, -0.0711, 0.0184, 0.0476, -0.0254])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWVrAeRjFA1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = diction.filter(diction.word == \"batman\")[\"vector\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-3y09DniFA1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = (\"This is text I want to know about\").split(\" \")\n",
        "                                                   \n",
        "                                \n",
        "doc = spark.createDataFrame([(sent,), (sent,)],[\"sentence\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mYBfcUnFA1i",
        "colab_type": "code",
        "colab": {},
        "outputId": "3b30be71-c5f4-403f-e9bf-61ba9d9dedf7"
      },
      "source": [
        "type(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCNhKuqHFA1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "worthing = diction.filter(diction.word == \"batman\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "waX70e5kFA1o",
        "colab_type": "code",
        "colab": {},
        "outputId": "25a481a1-c6c8-464b-e246-5d6d48215140"
      },
      "source": [
        "model.transform(doc).head().model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute '_jvm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-9f7f76f8fbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_defaults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mpair_defaults_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_defaults_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lkNl6gIFA1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec = worthing.collect()[0].vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7U3Ig3FA1t",
        "colab_type": "code",
        "colab": {},
        "outputId": "d602fe46-f5e2-4759-bc1c-9f26002f6d71"
      },
      "source": [
        "cosine_similarity([vec], [vec])\n",
        "#vector representing batman is 100 percent similar to itself."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P1_oNiuFA1v",
        "colab_type": "code",
        "colab": {},
        "outputId": "22782bbb-c774-4506-87a7-da7f634827ef"
      },
      "source": [
        "model.getVectors().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+\n",
            "|         word|              vector|\n",
            "+-------------+--------------------+\n",
            "|     cristino|[-0.1160977184772...|\n",
            "|    hoshizora|[-0.1109780073165...|\n",
            "|        mells|[-0.0570699460804...|\n",
            "|     17901791|[-0.0185965653508...|\n",
            "|        frane|[-0.1329791694879...|\n",
            "|        gawar|[-0.0898746177554...|\n",
            "|          ku|[-0.0321204923093...|\n",
            "|        toppo|[0.00813362747430...|\n",
            "|        shuvo|[-0.1562451720237...|\n",
            "|       hilyer|[-0.0437848530709...|\n",
            "|        cetti|[0.13472637534141...|\n",
            "|         koel|[-0.0291677471250...|\n",
            "|        10292|[0.03037112019956...|\n",
            "|rhynchostylis|[0.09231655299663...|\n",
            "|        1910s|[0.11900905519723...|\n",
            "|        19125|[0.00824300106614...|\n",
            "|         onam|[-0.0484162457287...|\n",
            "|      rahmani|[-0.2499719411134...|\n",
            "|      prinzip|[-0.0082170348614...|\n",
            "|       197077|[-0.0509997271001...|\n",
            "+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6_--k1MFA1x",
        "colab_type": "raw"
      },
      "source": [
        "either need to write a function that gets the dense vector for each word,and averages a list of words pairwise.\n",
        "\n",
        "numpy or linear alg in sklearn\n",
        "\n",
        "transform should do that already---eric's understanding is default behavior is it does the averaging for us--we tokenize sentence\n",
        "\n",
        "\n",
        "goa;\" vector representatiopm pf a reddit comment. that vec rep is the average of all the vectors for the words--removing stopwords, probably :)\n",
        "\n",
        "once we have ths, what do we do?\n",
        "\n",
        "\n",
        "how similar is our vector of a reddit comment to a vector of \"leader\" or \"winner\"? (and do it against negative words)\n",
        "--could be like sentiment analysis\n",
        "\n",
        "if we had positive/negative classifications already, we could run those thru--single point for the average of negative or positive\n",
        "\n",
        "we can compare differences\n",
        "we can find synonyms--metaphors\n",
        "\n",
        "can we identify the word that differentiates biden from trump? \n",
        "\n",
        "a vector is a direction...anda word\n",
        "\n",
        "find the vector for trump, and for biden. then with the vector we try to find the word\n",
        "\n",
        "\n",
        "look for themes around how people talk about each person.\n",
        "\n",
        "take single comment, create vector, look for synonyms\n",
        "rice, repeat.---count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzyvnPIFA1y",
        "colab_type": "code",
        "colab": {},
        "outputId": "ef1902c7-3e53-455a-8938-1d7b7ac05726"
      },
      "source": [
        "model.findSynonyms(\"doctor\", 6).select(\"word\", fmt(\"similarity\", 5).alias(\"similarity\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute '_jvm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-ad5b01dad8ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindSynonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doctor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mformat_number\u001b[0;34m(col, d)\u001b[0m\n\u001b[1;32m   1577\u001b[0m     \"\"\"\n\u001b[1;32m   1578\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ULDDhRcFA11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}