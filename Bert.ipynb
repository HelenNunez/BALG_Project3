{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjjleRj8jLz32HlSNVM6ss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HelenNunez/BALG_Project3/blob/SARA1/Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6-xj0SGEOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e379ada7-4ba0-4f96-f0e5-f0534000678d"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed -q pyspark==2.4.4\n",
        "! pip install --ignore-installed -q spark-nlp==2.5.5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_265\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 62kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 46.1MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSw39A-7GV4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "c421d421-c012-4188-e5c0-eb15bc82b557"
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version 2.5.5\n",
            "Apache Spark version: 2.4.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ac427baaa6b4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7c2ec33160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHja7tYWGrtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clf_lp(model_name, sentiment_dl=False, pretrained=True):\n",
        "\n",
        "  documentAssembler = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\n",
        "\n",
        "  use = UniversalSentenceEncoder.pretrained(lang=\"en\") \\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "  if pretrained:\n",
        "\n",
        "    if sentiment_dl:\n",
        "\n",
        "      document_classifier = SentimentDLModel.pretrained(model_name, 'en') \\\n",
        "                .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n",
        "                .setOutputCol(\"class\")\n",
        "    else:\n",
        "      document_classifier = ClassifierDLModel.pretrained(model_name, 'en') \\\n",
        "                .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n",
        "                .setOutputCol(\"class\")\n",
        "\n",
        "  else:\n",
        "\n",
        "    if sentiment_dl:\n",
        "\n",
        "      document_classifier = SentimentDLModel.load(model_name) \\\n",
        "                .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n",
        "                .setOutputCol(\"class\")\n",
        "    else:\n",
        "      document_classifier = ClassifierDLModel.load(model_name) \\\n",
        "                .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n",
        "                .setOutputCol(\"class\")\n",
        "\n",
        "  nlpPipeline = Pipeline(stages=[\n",
        "  documentAssembler, \n",
        "  use,\n",
        "  document_classifier\n",
        "  ])\n",
        "\n",
        "  empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "  clf_pipelineFit = nlpPipeline.fit(empty_data)\n",
        "\n",
        "  clf_lp_pipeline = LightPipeline(clf_pipelineFit)\n",
        "\n",
        "  return clf_lp_pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6AXYXfPGz5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_lp_pipeline = get_clf_lp('classifierdl_use_trec50')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M08gGJMG6tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'What was the number of member nations of the U.N. in 2000?'\n",
        "\n",
        "clf_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1US0_UgQG-sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_lp_pipeline.fullAnnotate(text)[0]['class'][0].result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db36hOpsG-17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_lp_pipeline.fullAnnotate(text)[0]['class'][0].metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPE-AyA1G--h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'What animal was the first mammal successfully cloned from adult cells?'\n",
        "\n",
        "clf_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhVQKroTHJst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_lp_pipeline = get_clf_lp('classifierdl_use_cyberbullying')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRF_hSvgHJ2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text ='RT @EBeisner @ahall012 I agree with you!! I would rather brush my teeth with sandpaper then watch football with a girl!!'\n",
        "\n",
        "clf_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMzOKplSHJ-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_lp_pipeline = get_clf_lp('classifierdl_use_fakenews')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbSdoYOEHKI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text ='Donald Trump a KGB Spy? 11/02/2016 In today’s video, Christopher Greene of AMTV reports Hillary Clinton campaign accusation that Donald Trump is a KGB spy is about as weak and baseless a claim as a Salem witch hunt or McCarthy era trial. It’s only because Hillary Clinton is losing that she is lobbing conspiracy theory. Citizen Quasar The way I see it, one of two things will happen: 1. Trump will win by a landslide but the election will be stolen via electronic voting, just like I have been predicting for over a decade, and the American People will accept the skewed election results just like they accept the TSA into their crotches. 2. Somebody will bust a cap in Hillary’s @$$ killing her and the election will be postponed. Follow AMTV!'\n",
        "\n",
        "clf_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AspvAGSBHcHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text ='Sen. Marco Rubio (R-Fla.) is adding a veteran New Hampshire political operative to his team as he continues mulling a possible 2016 presidential bid, the latest sign that he is seriously preparing to launch a campaign later this year.Jim Merrill, who worked for former GOP presidential nominee Mitt Romney and ran his 2008 and 2012 New Hampshire primary campaigns, joined Rubio’s fledgling campaign on Monday, aides to the senator said.Merrill will be joining Rubio’s Reclaim America PAC to focus on Rubio’s New Hampshire and broader Northeast political operations.\"Marco has always been well received in New Hampshire, and should he run for president, he would be very competitive there,\" Terry Sullivan, who runs Reclaim America, said in a statement. \"Jim certainly knows how to win in New Hampshire and in the Northeast, and will be a great addition to our team at Reclaim America.”News of Merrill’s hire was first reported by The New York Times.'\n",
        "\n",
        "clf_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9BBV7dBHcR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_lp_pipeline = get_clf_lp('sentimentdl_use_twitter', sentiment_dl=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGsqOXATHcZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text ='I am SO happy the news came out in time for my birthday this weekend! My inner 7-year-old cannot WAIT!'\n",
        "\n",
        "sentiment_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq77fErMHrvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_lp_pipeline = get_clf_lp('classifierdl_use_emotion', sentiment_dl=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzOERWHIHr4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_lp_pipeline.annotate(text)['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjMe5G2GH4Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xDuuCS5H5kN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwuwHS7H5xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"news_category_train.csv\")\n",
        "\n",
        "trainDataset.show(truncate=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZnWHkDwIVzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainDataset.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q-0YNKkIV2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "trainDataset.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L-0OnIMIV6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"news_category_test.csv\")\n",
        "\n",
        "\n",
        "testDataset.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxawbo3fIV8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainingData, testData) = trainDataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lBXiti4IV_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"description\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "    \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "glove_embeddings = WordEmbeddingsModel().pretrained() \\\n",
        " .setInputCols([\"document\",'lemma'])\\\n",
        " .setOutputCol(\"embeddings\")\\\n",
        " .setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"category\")\\\n",
        "  .setMaxEpochs(3)\\\n",
        "  .setEnableOutputLogs(True)\n",
        "  #.setOutputLogsPath('logs')\n",
        "\n",
        "clf_pipeline = Pipeline(\n",
        "    stages=[document_assembler, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            lemma, \n",
        "            glove_embeddings,\n",
        "            embeddingsSentence,\n",
        "            classsifierdl])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87eNbM7nIWB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default classifierDL params:\n",
        "\n",
        "maxEpochs -> 10,\n",
        "lr -> 5e-3f,\n",
        "dropout -> 0.5f,\n",
        "batchSize -> 64,\n",
        "enableOutputLogs -> false,\n",
        "verbose -> Verbose.Silent.id,\n",
        "validationSplit -> 0.0f,\n",
        "outputLogsPath -> \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-jHSaNIWEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train (8 min for 10 epochs)\n",
        "%%time\n",
        "\n",
        "clf_pipelineModel = clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnkfIHgNIWHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd ~/annotator_logs && ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Cx41jhIWKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat ~/annotator_logs/ClassifierDLApproach_f222663dfb2c.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TJ1OPWsIWNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the predictions on test Set\n",
        "\n",
        "preds = clf_pipelineModel.transform(testDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfqaaMoKIWP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = clf_pipelineModel.transform(testDataset)\n",
        "\n",
        "preds.select('category','description',\"class.result\").show(10, truncate=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtIzWgtsJKv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "# The result is an array since in Spark NLP you can have multiple sentences.\n",
        "# Let's explode the array and get the item(s) inside of result column out\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNOfH-tQJK5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report(preds_df['result'], preds_df['category']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVig6XimJLBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# actual content is inside description column\n",
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "# we can also use sentece detector here if we want to train on and get predictions for each sentence\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# the classes/labels/categories are in category column\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"category\")\\\n",
        "  .setMaxEpochs(5)\\\n",
        "  .setEnableOutputLogs(True)\n",
        "\n",
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        classsifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZbafWtkJLK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_pipelineModel = use_clf_pipeline.fit(trainDataset)\n",
        "# 5 epochs takes around 10 min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN9m8Mg_JcdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd ~/annotator_logs && ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmZE4b6kJcme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat ~/annotator_logs/ClassifierDLApproach_524181565fd1.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh1EF30dJpVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKKxxOh-JwDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('title_conference.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWP4MW1KJwLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.Conference.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFyaXDspJwOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"title_conference.csv\")\n",
        "\n",
        "(trainingData, testData) = trainDataset.randomSplit([0.8, 0.2], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpr3kZQQJwYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"Title\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "# we can also use sentece detector here if we want to train on and get predictions for each sentence\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# the classes/labels/categories are in category column\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "  .setInputCols([\"sentence_embeddings\"])\\\n",
        "  .setOutputCol(\"class\")\\\n",
        "  .setLabelColumn(\"Conference\")\\\n",
        "  .setMaxEpochs(20)\\\n",
        "  .setEnableOutputLogs(True)\n",
        "\n",
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        classsifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zxtVMmJpbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "use_pipelineModel = use_clf_pipeline.fit(trainingData)\n",
        "\n",
        "# 20 epochs takes around 22 seconds !"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAONX6AqKDvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = use_pipelineModel.transform(testData)\n",
        "\n",
        "preds.select('Title','Conference',\"class.result\").show(10, truncate=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmWbP4EKD16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "preds_df = preds.select('Conference','Title',\"class.result\").toPandas()\n",
        "\n",
        "# Let's explode the array and get the item(s) inside of result column out\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report(preds_df['result'], preds_df['Conference']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2cnboR-KLJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "preds = pd.DataFrame(confusion_matrix(preds_df['result'], preds_df['Conference']), columns = np.unique(preds_df['Conference']), index =  np.unique(preds_df['Conference']))\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}